#
# File Name: sysctl.conf
# File Path: /etc/sysctl.conf
#
# Author: J1n H4ng<jinhang@mail.14bytes.com>
# Date: April 11, 2024
#
# Last Editor: J1n H4ng<jinhang@mail.14bytes.com>
# Last Modified:  April 11, 2024
#
# Description: Linux 系统内核和网络优化设置
# Source URL:
#   - https://www.cnblogs.com/tingxin/p/12606166.html
#   - https://blog.csdn.net/alwaysbefine/article/details/123858239
#

# 魔法请求键，具体文档内容查看：https://www.kernel.org/doc/html/latest/translations/zh_CN/admin-guide/sysrq.html
# 0 - 完全不能使用 SysRq 键
# 1 - 可以使用 SysRq 键的所有功能
# >1 - 对于允许的 SysRq 键功能的比特掩码，具体查看文档内容
kernel.sysrq = 0

# 内核参数，用于决定是否在生成核心转储文件是是否包含进程的 PID 信息。
# 0 - 生成核心转储文件时不包含进程的 PID 信息
# 1 - 生成核心转储文件时包含进程的 PID 信息
# 这个参数用于区分不同的核心转储文件，使得在系统中存在多个转储文件时更容易区分它们的来源进程
# > 核心转储文件介绍：
# > 核心转储文件是在 Unix 和 类 Unix 系统上发生程序崩溃时生成的文件。当一个程序因为某种原因（比如内存错误、段错误等）异常终止时。
# > 操作系统会将程序的内存状态以及其他相关信息写入一个特殊的文件中，这个文件就是核心转储文件。
# > 核心转储文件包含了程序崩溃时的内存映像、程序计数器值、寄存器内容等信息，这些信息对于诊断程序崩溃的原因非常有用。
# > 通常，核心转储文件的生成是由操作系统内核自动完成的，但是可以通过配置操作系统的参数来控制核心转储文件的生成方式和存储位置。
# > 在一些系统中，核心转储文件可能会被用于调试分析程序崩溃的原因，或者用于改进程序的稳定性和可靠性。
kernel.core_uses_pid = 1
# 脚本内容:
# ------------Start Scripts------------
#
# #!/bin/bash
# if [! -d "/var/coredump" ]; then
#     mkdir /var/coredump
# fi
# gzip > "/var/coredump/$1"
#
# ------------End Scripts------------
#
# 最终在 /var/coredump 目录下生成 core_<线程名>_<线程ID>_<进程ID>_sig_<信号值>_time_<coredump时间>.gz 文件
# kernel.core_pattern = | /usr/bin/coredump_helper.sh core_%e_%I_%p_sig_%s_time_%t.gz

# 默认值为 16384(16KB)
# 用来控制系统中消息队列的最大字节数
# 消息队列是一种进程间通信的方式，用与在不同进程之间传递数据
# 适当调整这个值可以提高系统的性能和可靠性，但是需要根据具体的应用场景和系统资源来进行权衡和调整
# TODO: 为什么是 2^16 次方
kernel.msgmnb = 65536 # 2^16 次方
# 与 msgmnb 不同，msgmax 限制单个消息的大小而不是整个消息队列的大小
kernel.msgmax = 65536
# 用于定义单个共享内存段的最大值。设置应该足够大，能在一个共享片段下容纳整个 SGA
# 设置的过低可能会导致需要创建多个共享内存段，这样可能导致系统性能的下降
# 本质原因是在实例启动以及 ServerProcesses 创建的时候，多个小的共享内存片段可能会导致当时轻微的系统性能的降低，（在启动时需要创建多个虚拟地址段，在进程创建的时候要让进程对多个段进行“识别”，会有一些影响 ) ，但是其他时候都不会有影响
# 官方建议值：
# 32 位系统：可取最大值为 4GB - 1byte， 即为 4294967295。建议值为多于内存的一半，所以如果是 32 为系统，一般可取值为 4294967295 。 32 位系统对 SGA 大小有限制，所以 SGA 肯定可以包含在单个共享内存段中。
# 64 位系统：可取最大值为物理内值 - 1byte，建议值为多于物理内存的一半，一般取值大于 SGA_MAX_SIZE 即可，可以取物理内存 -1byte 。
# - 12G 时： 12 * 1024 * 1024 * 1024 - 1 = 12884901887
# - 16G 时： 16 * 1024 * 1024 * 1024 - 1 = 17179869183
# - 32G 时： 32 * 1024 * 1024 * 1024 - 1 = 34359738367
# - 64G 时： 64 * 1024 * 1024 * 1024 - 1 = 68719476735
# - 128G 时： 128 * 1024 * 1024 * 1024 - 1 = 137438953471
kernel.shmmax = 68719476735
# 控制可以使用的共享内存的总页数，Linux 的共享内存页大小为 4KB，共享内存段的大小都是共享内存页大小的整数倍
# 内存为 12G 时： 12 * 1024 * 1024 * 1024 / 4 * 1024 = 3145728
# 内存为 16G 时： 16 * 1024 * 1024 * 1024 / 4 * 1024 = 4194304
# 内存为 32G 时： 32 * 1024 * 1024 * 1024 / 4 * 1024 = 8388608
# 内存为 64G 时： 64 * 1024 * 1024 * 1024 / 4 * 1024 = 16777216
# 内存为 128G 时: 128 * 1024 * 1024 * 1024 / 4 * 1024 = 33554432
# 官方计算为 ceil(shmmax / PAGE_SIZE )，ceil 为向上取整函数
kernel.shmall = 16777216
# 可以创建的共享内存段的最大数量
# TODO: kernel.shmmni 如何计算
kernel.shmmni = 4096 # 默认值

# 内核参数，用于限制系统可以分配的进程ID(PID)的最大值
# PID 是用来唯一标识每个正在运行的进程的数字标识符。
# 通过限制 kernel.pid_max 的值，系统可以控制能够分配的 PID 的数量，这个数量通常是为了防止 PID 的用尽
# 在 32 位系统上： PID 的范围是 0 - 2^15-1
# 在 64 位系统上： PID 的范围是 0 - 2^22-1
# TODO: PID 的计算方式
kernel.pid_max = 1000000

# 旧的：每 4M RAM，有 256 个
# 内存为 16G 时： 16 * 1024 * 256 / 4 = 1048576
# 内存为 32G 时： 32 * 1024 * 256 / 4 = 2097152
# 新的：每 1M RAM，有 1 个
# 内存为 16G 时： 16 * 1024 * 100 = 1638400
# 内存为 32G 时： 32 * 1024 * 100 = 3276800
# 默认设置为内存(KB)的 10%
fs.file-max = 1638400

# 内核参数，用于控制是否运行卸载已经分离的挂载点
# 0 - 不允许
# 1 - 允许
# 设置为 0 时不允许 umount 命令
fs.may_detach_mounts = 1

# 内核参数，用于限制单个用户可以创建的 inotify 实例的最大数量
# inotify 是 Linux 内核提供的一种文件系统监控机制，可以监视文件系统的变化
fs.inotify.max_user_instances = 524288

# 内核参数，用于控制内存过度承诺策略。内存过度承诺策略允许进程分配比实际可用物理内存更多的内存量。
# 可以提供系统的内存利用率，但也可以导致内存耗尽和进程被杀死的风险
# 0 - 启用经典的内存过度承诺策略，内核尝试估算出剩余可用的内存
# 1 - 总是过度投入，内核允许超量使用内存直到用完为止，主要用于科学计算
# 2 - 不要过度投入
# Redis 设置为 1
vm.overcommit_memory = 0
# vm.overcommit_memory = 2 时取消注释，value 值为 0-100，内存总量为：内存 * (value/100) + SWAP 分区大小，默认值为 50
# vm.overcommit_ratio = <value>

# 内核参数，用于控制系统对 SWAP 空间的使用倾向，默认值为 60，大小可以设置为 0 - 200
# 0 - 表示尽可能少的使用 SWAP
# 100 - 表示更倾向于使用 SWAP
# >100 - 磁盘交互比内存快时使用，如果快2倍，计算值为 2x + x = 200（两个设备 100 的和）, x = 133.33，值为 133.33
vm.swappiness = 0

# 内核参数，定义了一个进程可以拥有的内存映射区域的最大数量，通常用于限制一个进程可以打开的文件数量
# TODO: vm.max_map_count = 262144 的值如何计算出来
# Elasticsearch 启动设置
vm.max_map_count = 262144

# 内核参数，用于控制 IP 数据包的转发，在单个主机作为终端节点而不是路由器或网关时使用
# 0 - 禁用 IP 数据包转发
# 1 - 启用 IP 数据包转发
net.ipv4.ip_forward = 0

# all 的设置会覆盖 default 的设置
# 如果希望统一所有的网络接口的反向路径过滤设置，使用 all
# 如果只想设置新创建的网络接口是，使用 default
# 0 - 禁用反向路径过滤
# 1 - 启用反向路径过滤且对于不是同一接口出去的数据包，将其丢弃
# 2 - 表示严格模式的反向路径过滤，只有当接受到的数据包的源地址能够通过任意可用的接口发出时，才允许通过
# 默认值为 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.rp_filter = 1
# 忽略 ICMP 请求，出于安全考虑，建议开启此项，即禁 ping
# 0 - 不忽略 ICMP 请求
# 1 - 忽略 ICMP 请求
net.ipv4.icmp_echo_ignore_all = 0
# 表示系统将忽略收到的发送到广播地址的 ICMP 的回显请求，不做任何响应，意味着系统不会回应任何发送到广播地址的 ping 请求
# 0 - 不忽略
# 1 - 忽略
net.ipv4.icmp_echo_ignore_broadcasts = 0

# 内核参数，用于控制系统是否接受 IP 数据包中指定的源路由选项
# 0 - 表示系统将忽略 IP 数据包中指定的源路由选项
# 1 - 表示系统将接受 IP 数据包中指定的源路由选项
# 设置为 0 时，是一个安全的选项
net.ipv4.conf.default.accept_source_route = 0

# 该参数与性能无关，用于解决 TCP 的 SYN 攻击
net.ipv4.tcp_syncookies = 1

# 内核参数，用于设置 TIME-WAIT 状态的 TCP 连接的最大数量
# 在 TCP 协议中，当一条连接关闭时，它会进入 TIME-WAIT 状态，等待两倍的报文最大生存时间（MSL）后才会完全关闭
# 在 TIME-WAIT 状态下的连接不会占用太多的系统资源，但是如果短时间内产生了大量的连接关闭，TIME-WAIT 状态可能累积起来，导致资源浪费
# 用于控制系统中 TIME-WAIT 状态连接的数量上限。
# 用于防止 Dos 攻击
net.ipv4.tcp_max_tw_buckets = 6000
# 内核参数，用于控制系统是否启用 TCP TIME-WAIT 状态的快速回收机制
# 启用此选项后，内核会尝试在 TIME-WAIT 状态的连接上进行时间戳检测，并将相同时间戳的连接视为重复连接，这样系统在处理大量短周期连接时，更快的回收 TIME-WAIT 状态的连接资源
# 启用后可能导致一些特定网络拓扑下的连接问题，尤其上当连接穿越了对称 NAT 网关时
# 0 - 禁用
# 1 - 启用
net.ipv4.tcp_tw_recycle = 0
# 内核参数，用于控制系统是否启用 TCP 时间戳选项
# 0 - 禁用
# 1 - 启用
net.ipv4.tcp_timestamps = 0
# 内核参数，用于控制系统是否启用 TCP TIME-WAIT 状态的端口重用机制
# 启用后，内核会允许在 TIME-WAIT 状态的连接关闭后，立即重用该端口，这样可以更有效的利用系统资源
# 可能会导致一些复用的问题，特别是当之前的连接尚未完全关闭或连接复用不当时
# 0 - 禁用
# 1 - 启用
net.ipv4.tcp_tw_reuse = 1
# 内核参数，用于设置 TCP 连接的空闲超时时间，单位为秒
net.ipv4.tcp_keepalive_time = 1200
# 是否启用有选择的应答，可以通过有选择的应答乱序接受到的报文来提高性能，这样可以让发送者只发送丢失的报文段
# 对于广域网来说这个选项应该启用，但是会增加 CPU 的占用
net.ipv4.tcp_sack = 1
# 内核参数，用于控制是否启用 TCP 窗口缩放选项
# TCP 窗口缩放是一种 TCP 拓展机制，用于解决 TCP 的窗口大小受限问题，在传统的 TCP 中，TCP 的窗口大小是 0-65535，这个大小对于高速网络来说可能会限制 TCP 的性能
# TCP 窗口缩放允许发送方和接收方在 TCP 连接建立过程中协商一个更大的窗口大小，以适应更高带宽的网络
net.ipv4.tcp_window_scaling = 1
# 内核参数，用于设置 TCP 接收缓冲区的大小，TCP 接收缓冲区用于存储接收到的数据，等待应用程序来读取。
# 由三个数字组成，分别表示最小值，默认值和最大值
net.ipv4.tcp_rmem = 4096 131072 6291456
# 内核参数，用于设置 TCP 发送缓冲区的大小，TCP 发送缓冲区用于存储待发送的数据，等待发送到网络上。
# 这个参数由三个数字组成，分别表示最小值、默认值和最大值
# 4096（4KB） 是最小缓存区大小，默认情况下与系统页大小相同，是为了确保即便在低内存机器上或者存在内存压力的情况下也有缓存
# 16384 （16KB）是默认大小，系统上所有 TCP 套接字缓冲区默认大小，
#   会覆盖 net.core.wmem_default 的大小（只对 TCP 协议生效），net.core.wmem_default 是所有网络协议（TCP、UDP）的默认发送缓冲区大小
# 4194304 （4MB）是最大缓冲区大小，与默认大小不同，该值不会覆盖 net.core.wmem_max 的大小
net.ipv4.tcp_wmem = 4096 16384 4194304
# 内核参数，用于设置 TCP 协议的内存使用策略，而不是单独的缓冲区大小。
net.ipv4.tcp_mem = 94500000 915000000 927000000

# net.core.wmem_default 是所有网络协议（TCP、UDP）的默认发送缓冲区大小
net.core.wmen_default = 8388608
# net.core.wmem_default 是所有网络协议（TCP、UDP）的最大发送缓冲区大小
net.core.wmem_max = 16777216
# net.core.rmem_default 是所有网络协议（TCP、UDP）的默认接收缓冲区大小
net.core.rmem_default = 8388608
# net.core.rmem_max 是所有网络协议（TCP、UDP）的最大接收缓冲区大小
net.core.rmem_max = 16777216

# 内核参数，用于设置本地端口号的范围，本地端口号是由客户端程序或服务端程序在发起网络连接或监听网络连接时所使用的端口号
# 第一个数字表示本地端口号的最小值
# 第二个数字表示本地端口号的最大值
# 0-1023 是系统保留端口号，用于一些常见的网络服务，故不会影响 nginx 监听的 80，443 端口
net.ipv4.ip_local_port_range = 1024 65000

# 内核参数，用于设置路由缓存项的过期超时时间
# 路由缓存用于存储系统中已知的网络路由信息，以便在转发数据包时快速查找目的地址的路由信息。
# 当路由缓存中的条目长时间未被使用时，系统可以选择将其从缓存中删除，以释放资源以保持路由表的更新
net.ipv4.route.gc_timeout = 300

# 内核参数，用于设置 TCP 连接的 FIN-WAIT-2 状态的超时时间
# 在 TCP 协议中，当一方发送 FIN 报文以表示连接关闭后，它进入到 FIN-WAIT-1 状态，等待另一方确认
# 另一方收到 FIN 报文后，发送确认后进入 CLOSE-WAIT 状态，并开始关闭连接，而发送了确认一方进入到 FIN-WAIT-2 状态，在此状态下等待一段时间以确保另一方已经完全关闭连接
# 指定了在 FIN-WAIT-2 状态下等待的超时时间，单位为秒
net.ipv4.tcp_fin_timeout = 30

# 内核参数，用于设置网络设备接收队列的最大长度
# 网络设备接收队列用于暂时存储从网络接口接收到的数据包，这些数据包等待倍内核进程接收和处理，当接收队列已满时，新到达的数据包将被丢弃或倍网络设备本地处理
# 单位是数据包数量，它影响系统在高网络负载情况下的性能和稳定性，增加这个值可以提高系统对网络流量的处理能力，但也会增加系统的内存消耗和延迟。
net.core.netdev_max_backlog = 262144
# 内核参数，用于设置套接字的最大排队连接数
# 当服务器接收到大量连接请求时，这些连接请求会排队等待被服务器接受。
# 当排队连接数达到这个值时，新的连接请求将被拒绝或者延迟处理，直到有空闲的连接队列位置
net.core.somaxconn = 65535

# 内核参数，用于设置系统中允许的最大孤立 TCP 连接数
# 孤立连接是指客户端发起了连接但是没有完全建立起连接，或者连接已经被关闭但是仍然存在于系统中等待被清理的连接
# 这些孤立连接会占用系统资源，包括内存和文件描述符等，如果数量过多，可能会导致资源耗尽和系统性能下降
net.ipv4.tcp_max_orphans = 3276800
# 内核参数，用于设置 TCP 半连接队列的最大长度
# 在 TCP 三次握手过程中，服务器在接收到客户端的 SYN 请求后，会将这个连接放入到半连接队列中等待三次握手的完成
# 这个值指定了可以容纳的等待完成三次握手的连接的最大数量
# 当半连接队列已满时，新的 SYN 请求将被拒绝或延迟处理，直到有空闲的半连接队列位置，这个参数的设置可以影响系统处理新连接的速度和并发连接数
net.ipv4.tcp_max_syn_backlog = 262144
# 内核参数，用于设置 TCP 连接建立中的 SYN 的尝试次数
net.ipv4.tcp_syn_retries = 1
# 内核参数，用于设置 TCP 连接建立在的 SYN+ACK 的尝试次数
net.ipv4.tcp_synack_retries = 1

# 是否禁用 IPv6 协议
# 0 - 不禁用
# 1 - 禁用
# 所有的网络接口
net.ipv6.conf.all.disable_ipv6 = 0
# 新增的网络接口
net.ipv6.conf.default.disable_ipv6 = 0

# 连接跟踪表设置
# 内核参数，用于设置连接跟踪表的最大容量
# 连接跟踪表：是 Linux 内核中的一个功能，用于跟踪网络连接的状态，当数据包经过 Linux 系统时，连接跟踪会检测数据包的状态，并根据需要更新连接跟踪表中的信息
# 连接跟踪表用于存储当前活动的网络连接信息，包括源地址、目标地址、协议类型、端口号等
# net.nf_conntrack_max 和 net.netfilter.nf_conntrack_max 是同一个参数的两种别名
# 但是它们具有相同的功能和作用
# 默认计算公式为：
# 8G：8 * 1024 * 1024 * 1024 / 16384（HASH 表大小） /（ 64（64位机器）/32） = 262144
net.netfilter.nf_conntrack_max = 262144

# net.netfilter.nf_conntrack_buckets = net.netfilter.nf_contrack_max / 4 (不能直接改)
# 临时生效：
# 8G：echo 262144 > /sys/module/nf_conntrack/parameters/hashsize
# 重启永久生效
# vim /etc/modprobe.d/iptables.conf
# options nf_conntrack hashsize = 262144

# 内核参数，用于设置 ICMP 连接的超时时间
net.netfilter.nf_conntrack_icmp_timeout = 10

net.netfilter.nf_conntrack_tcp_timeout_syn_recv = 5
net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 5

# 内核参数，用于设置 TCP 连接在 CLOSE_WAIT 下的超时时间
# 默认值： 60
net.netfilter.nf_conntrack_tcp_timeout_close_wait = 10
# 内核参数，用于设置 TCP 连接在 FIN_WAIT 下的超时时间
# 默认值： 60
net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 10
# 内核参数，用于设置 TCP 连接在 TIME_WAIT 下的超时时间
# 默认值： 120
net.netfilter.nf_conntrack_tcp_timeout_time_wait = 10
# 默认值： 60
net.netfilter.nf_conntrack_tcp_timeout_close_wait = 20
# 默认值： 30
net.netfilter.nf_conntrack_tcp_timeout_last_ack = 10
# 内核参数，用于已经建立的 TCP 连接的超时时间，当连接在这个时间内没收到数据包时，内核会自动关闭这个连接
net.netfilter.nf_conntrack_tcp_timeout_established = 3600
